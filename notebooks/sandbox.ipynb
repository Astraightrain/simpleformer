{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokens = tokenizer.encode(sentence, return_tensors='pt', padding=\"max_length\", max_length=20)\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    return model.embeddings.word_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = get_embedding('my name is jungwoo')\n",
    "x2 = get_embedding('hi bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = torch.cat([x1, x2])\n",
    "batch_sample.size()\n",
    "\n",
    "d_model = 768\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_model: int, \n",
    "            dropout: float, \n",
    "            max_length: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_length, d_model)\n",
    "        positions = rearrange(torch.arange(0, max_length, dtype=torch.float), 'm -> m 1')\n",
    "\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)) / d_model) # 1000^(2i/dim_model)\n",
    "\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions * division_term)\n",
    "\n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        self.pos_encoding = rearrange(pos_encoding, 'm d -> m 1 d')\n",
    " \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(d_model, 0.1, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3281e-02,  1.0974e+00, -0.0000e+00,  ...,  1.1785e+00,\n",
       "           2.5599e-02,  1.1376e+00],\n",
       "         [ 1.5603e-02,  0.0000e+00, -6.7736e-02,  ...,  1.0172e+00,\n",
       "          -0.0000e+00,  1.1001e+00],\n",
       "         [-1.8944e-04,  1.1115e+00, -4.2494e-02,  ...,  1.0638e+00,\n",
       "          -3.2619e-02,  1.0424e+00],\n",
       "         ...,\n",
       "         [-1.8499e-02,  1.0371e+00, -1.8143e-02,  ...,  1.0889e+00,\n",
       "          -5.7111e-02,  1.0818e+00],\n",
       "         [-1.8499e-02,  1.0371e+00, -1.8143e-02,  ...,  1.0889e+00,\n",
       "          -5.7111e-02,  1.0818e+00],\n",
       "         [-1.8499e-02,  1.0371e+00, -1.8143e-02,  ...,  1.0889e+00,\n",
       "          -5.7111e-02,  1.0818e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  5.8665e-01,  8.9732e-01,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  1.1376e+00],\n",
       "         [ 8.9727e-01,  4.9398e-01,  9.7411e-01,  ...,  1.0203e+00,\n",
       "          -1.2909e-01,  0.0000e+00],\n",
       "         [ 1.0154e+00,  5.0047e-01,  9.6167e-01,  ...,  1.1025e+00,\n",
       "          -2.8866e-02,  1.0463e+00],\n",
       "         ...,\n",
       "         [ 9.1647e-01,  5.2632e-01,  9.0234e-01,  ...,  1.0889e+00,\n",
       "          -5.6997e-02,  1.0818e+00],\n",
       "         [ 9.1647e-01,  5.2632e-01,  9.0234e-01,  ...,  1.0889e+00,\n",
       "          -5.6997e-02,  1.0818e+00],\n",
       "         [ 9.1647e-01,  0.0000e+00,  9.0234e-01,  ...,  1.0889e+00,\n",
       "          -5.6997e-02,  1.0818e+00]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(batch_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
