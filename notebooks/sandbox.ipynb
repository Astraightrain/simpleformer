{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "    tokens = tokenizer.encode(sentence, return_tensors='pt', padding=\"max_length\", max_length=20)\n",
    "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    return model.embeddings.word_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = get_embedding('my name is jungwoo')\n",
    "x2 = get_embedding('hi bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = torch.cat([x1, x2])\n",
    "batch_sample.size()\n",
    "\n",
    "d_model = 768\n",
    "max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            d_model: int, \n",
    "            dropout: float, \n",
    "            max_length: int,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_length, d_model)\n",
    "        positions = rearrange(torch.arange(0, max_length, dtype=torch.float), 'm -> m 1')\n",
    "\n",
    "        division_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)) / d_model) # 1000^(2i/dim_model)\n",
    "\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions * division_term)\n",
    "\n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        self.pos_encoding = rearrange(pos_encoding, 'm d -> m 1 d')\n",
    " \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(d_model, 0.1, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_embedding = pe(batch_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MultiHeadAttention(d_model=d_model, nheads=8, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -0.1108, -0.0000,  ..., -0.0000, -0.2244,  0.1460],\n",
       "         [-0.4496, -0.1814, -0.0760,  ..., -0.3067, -0.2866,  0.0262],\n",
       "         [-0.0000, -0.1287, -0.1443,  ..., -0.2137, -0.1096,  0.0155],\n",
       "         ...,\n",
       "         [-0.4968, -0.0000, -0.2834,  ..., -0.1626, -0.1893,  0.0697],\n",
       "         [-0.4083, -0.0000, -0.2405,  ..., -0.2550, -0.0000,  0.0725],\n",
       "         [-0.3810, -0.1904, -0.0164,  ..., -0.2757, -0.1953,  0.1651]],\n",
       "\n",
       "        [[-0.4527, -0.0000, -0.2503,  ..., -0.3200, -0.4835,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.3693,  ..., -0.3026, -0.5328,  0.0376],\n",
       "         [-0.3926, -0.2420, -0.2051,  ..., -0.3418, -0.2692,  0.0199],\n",
       "         ...,\n",
       "         [-0.4813, -0.1311, -0.3769,  ..., -0.2857, -0.4601, -0.0304],\n",
       "         [-0.4268, -0.0524, -0.3326,  ..., -0.0000, -0.4997, -0.0761],\n",
       "         [-0.4626, -0.0930, -0.0000,  ..., -0.3061, -0.4862,  0.0489]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(batch_embedding, batch_embedding,batch_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            temperature: float = 0.1,\n",
    "            attn_dropout: float = 0.1\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            mask = None   \n",
    "    ):\n",
    "        d_k = q.size(0)\n",
    "        k = rearrange(k, 'b n m d -> b n d m')\n",
    "        attn = torch.matmul(q / self.temperature, k)\n",
    "        attn = torch.div(attn, np.sqrt(d_k))\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = F.softmax(attn, dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output \n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    b: batch size\n",
    "    m: max_seq length\n",
    "    n: nheads\n",
    "    h: head_dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        nheads,\n",
    "        dropout: float = 0.1,\n",
    "        attn_dropout: float = 0.1,\n",
    "        bias = True,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nheads = nheads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        assert self.d_model % nheads == 0\n",
    "\n",
    "        self.q = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.k = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.v = nn.Linear(d_model, d_model, bias = bias)\n",
    "        self.selfattn = SelfAttention(\n",
    "            temperature = 0.1,\n",
    "            attn_dropout = attn_dropout\n",
    "        )\n",
    "        self.o = nn.Linear(d_model, d_model, bias = bias)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            src,\n",
    "            src_mask = None\n",
    "    ):\n",
    "        # MultiheadAttention\n",
    "        q = rearrange(self.q(src), 'b m (n h) -> b n m h', n = self.nheads)\n",
    "        k = rearrange(self.k(src), 'b m (n h) -> b n m h', n = self.nheads)\n",
    "        v = rearrange(self.k(src), 'b m (n h) -> b n m h', n = self.nheads)\n",
    "\n",
    "        output = self.selfattn(q, k, v, mask = src_mask)\n",
    "        output = rearrange(output, 'b n m h -> b m (n h)', n = self.nheads)\n",
    "        output = self.o(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        dim_feedforward: int,\n",
    "        activation = nn.SiLU(),\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            activation,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feedforward(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.layer(x, **kwargs) + x\n",
    "\n",
    "class PostNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, d_model):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.d_model = d_model\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.layernorm(self.layer(x, **kwargs))\n",
    "    \n",
    "class PreNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, d_model):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.d_model = d_model\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.layer(self.layernorm(x), **kwargs)\n",
    "\n",
    "class VanillaEncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "            d_model, \n",
    "            nheads, \n",
    "            dim_feedforward,  \n",
    "            dropout,\n",
    "            attn_dropout \n",
    "                 \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = PostNormalization(\n",
    "            ResidualConnection(\n",
    "                MultiHeadAttention(\n",
    "                d_model = d_model, \n",
    "                nheads = nheads, \n",
    "                dropout = dropout, \n",
    "                attn_dropout = attn_dropout\n",
    "                )\n",
    "            ),\n",
    "            d_model = d_model\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.feedforward = PostNormalization(\n",
    "            ResidualConnection(\n",
    "                FeedForwardBlock(\n",
    "                d_model = d_model,\n",
    "                dim_feedforward = dim_feedforward,\n",
    "                dropout = dropout\n",
    "                )\n",
    "            ),\n",
    "            d_model = d_model\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask = None):\n",
    "        \n",
    "\n",
    "        #self attention & residual connection\n",
    "        output = self.attn(src, src_mask)\n",
    "        output = self.feedforward(output)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model//8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_layer = VanillaEncoderLayer(d_model=d_model, nheads=n_heads, dim_feedforward=1024,dropout=0.1, attn_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PostNormalization.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JWK\\Desktop\\projects\\simpleformer\\notebooks\\sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m enc_layer(batch_embedding)\n",
      "File \u001b[1;32mc:\\Users\\JWK\\anaconda3\\envs\\develop\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JWK\\anaconda3\\envs\\develop\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\JWK\\Desktop\\projects\\simpleformer\\notebooks\\sandbox.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, src_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m     \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     \u001b[39m#self attention & residual connection\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(src, src_mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(output)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/JWK/Desktop/projects/simpleformer/notebooks/sandbox.ipynb#X41sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m src\n",
      "File \u001b[1;32mc:\\Users\\JWK\\anaconda3\\envs\\develop\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JWK\\anaconda3\\envs\\develop\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PostNormalization.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "enc_layer(batch_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "head_dim = d_model // n_heads\n",
    "Q = batch_embedding.view(2, -1, n_heads, head_dim).permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 20, 96])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 20, 96])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(batch_embedding, 'b m (n h) -> b n m h', n=n_heads).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 768])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
